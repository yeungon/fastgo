# High-Concurrency Go Web Server

Production-ready Go HTTP server designed to handle tens of thousands of concurrent connections efficiently, inspired by the principles from "Handling 60K Concurrent HTTP Requests on Raspberry Pi."

## ðŸš€ Key Features

- **FastHTTP Integration**: 35% less memory usage compared to standard `net/http`
- **Worker Pool Pattern**: Efficient CPU utilization with configurable worker count
- **Graceful Shutdown**: Proper cleanup and connection draining
- **Metrics & Monitoring**: Real-time performance statistics with SSE streaming
- **Real-Time Dashboard**: Live charts comparing server performance
- **Production-Ready**: Timeouts, error handling, and resource limits
- **Scalable Architecture**: Handles 60K+ concurrent connections
- **Triple Server Architecture**: Worker Pool (FastHTTP) + Fiber (FastHTTP) + Chi (net/http) for fair comparison

## ðŸ“Š Benchmark Results (VPS - 2 CPU, 4GB RAM)

| Metric | Worker Pool (8080) | Chi Web (8081) | Fiber (8082) |
|--------|-------------------|----------------|--------------|
| **HTTP Library** | FastHTTP | net/http | FastHTTP |
| **Throughput (root)** | 9.84 req/sec* | 1,156 req/sec | ~1,200 req/sec |
| **Throughput (health)** | 2,532 req/sec | 2,378 req/sec | ~2,600 req/sec |
| **Max Concurrent Users** | 2,000+ | 2,000+ | 2,000+ |
| **Avg Latency** | ~110ms* | ~12ms | ~10ms |
| **P95 Latency** | ~290ms | ~50ms | ~45ms |

*Worker Pool includes 100ms simulated work; Chi Web & Fiber have 10ms simulated work

### k6 Load Test Results (2000 Concurrent Students)

```
âœ“ Total Requests:     217,880
âœ“ Throughput:         885 req/sec
âœ“ Avg Latency:        116.82ms
âœ“ P95 Latency:        290.69ms
âœ“ Success Rate:       100% (GET) / 0% (POST - no handler)
âœ“ Max VUs:            2,000
```

## ðŸ“‹ System Requirements & Limits

### Linux File Descriptor Limits

```bash
# Check current limits
ulimit -n

# Temporary increase (current session)
ulimit -n 1000000

# Permanent increase - add to /etc/security/limits.conf
* soft nofile 1000000
* hard nofile 1000000

# Verify after reboot
ulimit -n
```

### TCP Port Range (Client-side)

```bash
# Check current port range
sysctl net.ipv4.ip_local_port_range

# Increase ephemeral port range (temporary)
sudo sysctl -w net.ipv4.ip_local_port_range="1024 65535"

# Make permanent - add to /etc/sysctl.conf
net.ipv4.ip_local_port_range = 1024 65535

# Apply changes
sudo sysctl -p
```

### MacOS Limits

```bash
# File descriptors
sudo sysctl -w kern.maxfiles=1000000
sudo sysctl -w kern.maxfilesperproc=1000000
ulimit -S -n 1000000

# Port range
sudo sysctl -w net.inet.ip.portrange.first=1024
sudo sysctl -w net.inet.ip.portrange.hifirst=1024
```

## ðŸ”§ Installation

```bash
# Clone repository
git clone https://github.com/yeungon/fastgo.git
cd highconcurrency-server

# Install dependencies
go mod download

# Build all three servers
make build-all

# Run all three servers
make run-all
```

### Quick Start

```bash
# Build and run worker pool server only
go build -o server main.go
./server

# Build and run chi web server only
go build -o web-server cmd/web/main.go
PORT=8081 ./web-server

# Build and run fiber server only
go build -o fiber-server cmd/fiber/main.go
./fiber-server

# Or use Makefile
make run-all    # Run all 3 servers
make run-both   # Run Worker Pool + Chi only
make stop-all   # Stop all servers
```

## ðŸŽ¯ Usage

### Basic Startup

```bash
# Default configuration (port 8080, CPU cores * 2 workers)
./server

# Custom port
PORT=3000 ./server

# Custom worker count
WORKERS=8 ./server

# Combined
PORT=3000 WORKERS=16 ./server
```

### API Endpoints

All three servers expose the same endpoints:

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | Main endpoint (with simulated work) |
| `/health` | GET | Health check |
| `/metrics` | GET | JSON metrics |
| `/dashboard` | GET | Real-time metrics dashboard |
| `/compare` | GET | 2-server comparison (Worker Pool vs Chi) |
| `/compare3` | GET | 3-server comparison (all servers) |
| `/sse/metrics` | GET | Server-Sent Events stream |

#### Process Request
```bash
# Worker Pool Server (100ms work)
curl http://localhost:8080/

# Chi Web Server (10ms work)
curl http://localhost:8081/

# Fiber Server (10ms work)
curl http://localhost:8082/
```

#### Health Check
```bash
curl http://localhost:8080/health
curl http://localhost:8081/health
```

#### Real-Time Dashboard
Open in browser:
- Worker Pool: http://localhost:8080/dashboard
- Chi Web: http://localhost:8081/dashboard
- Fiber: http://localhost:8082/dashboard
- **2-Server Comparison**: http://localhost:8080/compare
- **3-Server Comparison**: http://localhost:8080/compare3

#### Metrics
```bash
curl http://localhost:8080/metrics
```

Response:
```json
{
  "active_connections": 150,
  "total_requests": 10000,
  "completed_requests": 9850,
  "error_count": 5,
  "uptime_seconds": 120.5,
  "requests_per_sec": 81.74,
  "memory_alloc_mb": 245,
  "memory_sys_mb": 512,
  "num_goroutines": 250,
  "num_gc": 42
}
```

## ðŸ“Š Load Testing

### Quick Start: Testing All 3 Servers

```bash
# 1. Build and run all three servers
make run-all

# 2. Open the comparison dashboard in browser
open http://localhost:8080/compare3

# 3. Run k6 load tests on all servers simultaneously
k6 run --duration 30s --vus 100 loadtest/k6-quick.js &
k6 run --duration 30s --vus 100 loadtest/k6-quick.js --env TARGET_URL=http://localhost:8081 &
k6 run --duration 30s --vus 100 loadtest/k6-quick.js --env TARGET_URL=http://localhost:8082 &

# 4. Stop all servers when done
make stop-all
```

### Real-Time Dashboards

| Dashboard | URL | Description |
|-----------|-----|-------------|
| Single Server | http://localhost:8080/dashboard | Worker Pool metrics only |
| Dual Comparison | http://localhost:8080/compare | Worker Pool vs Chi Web |
| **Triple Comparison** | http://localhost:8080/compare3 | **All 3 servers side-by-side** |

### Using k6 (Recommended)

```bash
# Install k6
# macOS: brew install k6
# Ubuntu: apt install k6
# Windows: choco install k6

# Test individual servers
k6 run loadtest/k6-quick.js                                              # Worker Pool (8080)
k6 run loadtest/k6-quick.js --env TARGET_URL=http://localhost:8081       # Chi Web (8081)
k6 run loadtest/k6-quick.js --env TARGET_URL=http://localhost:8082       # Fiber (8082)

# Student registration simulation (2000 users)
k6 run loadtest/k6-student-registration.js

# Stress test (find breaking point)
k6 run loadtest/k6-stress.js

# Compare Worker Pool vs Chi Web
k6 run loadtest/k6-compare-servers.js
```

### Testing Remote VPS

```bash
# Quick test each server on VPS
k6 run loadtest/k6-quick.js --env TARGET_URL=http://YOUR_VPS_IP:8080  # Worker Pool
k6 run loadtest/k6-quick.js --env TARGET_URL=http://YOUR_VPS_IP:8081  # Chi Web
k6 run loadtest/k6-quick.js --env TARGET_URL=http://YOUR_VPS_IP:8082  # Fiber

# 2000 user simulation on VPS
k6 run loadtest/k6-student-registration.js --env TARGET_URL=http://YOUR_VPS_IP:8081

# Stress test VPS
k6 run loadtest/k6-stress.js --env TARGET_URL=http://YOUR_VPS_IP:8081

# Compare both VPS servers
k6 run loadtest/k6-compare-servers.js --env HOST=YOUR_VPS_IP

# Custom VUs (1000 concurrent connections)
k6 run loadtest/k6-quick.js --env TARGET_URL=http://YOUR_VPS_IP:8081 --env VUS=1000

# Custom VUs with duration override
k6 run -u 1000 -d 60s loadtest/k6-quick.js --env TARGET_URL=http://YOUR_VPS_IP:8081
```

### Available k6 Scripts

| Script | Description | Max VUs | Duration |
|--------|-------------|---------|----------|
| `k6-quick.js` | Quick development test | 200 | 60s |
| `k6-student-registration.js` | Realistic user flow | 2,000 | 5min |
| `k6-stress.js` | Find breaking point | 10,000 | 12min |
| `k6-compare-servers.js` | Compare both servers | 1,000 | 3min |

### Using wrk

```bash
# Install wrk
# Ubuntu: sudo apt install wrk
# macOS: brew install wrk

# Test for 30 seconds with 1000 connections, 8 threads
wrk -t8 -c1000 -d30s http://localhost:8080/

# Test Chi Web server
wrk -t8 -c1000 -d30s http://localhost:8081/

# Quick health check benchmark
wrk -t4 -c100 -d10s http://localhost:8080/health
```

### Using Apache Bench

```bash
# Test with 10K requests, 1K concurrent
ab -n 10000 -c 1000 http://localhost:8080/

# With keep-alive
ab -n 10000 -c 1000 -k http://localhost:8080/
```

### Using Go Client (from article)

```go
package main

import (
	"fmt"
	"net/http"
	"time"
)

func main() {
	tr := &http.Transport{
		ResponseHeaderTimeout: time.Hour,
		MaxConnsPerHost:       99999,
		DisableKeepAlives:     true,
	}

	client := &http.Client{Transport: tr}

	for i := 0; i < 60000; i++ {
		go func(n int) {
			resp, err := client.Get("http://localhost:8080/")
			if err != nil {
				fmt.Printf("%d: %s\n", n, err.Error())
				return
			}
			defer resp.Body.Close()
		}(i)
		
		time.Sleep(1 * time.Millisecond)
		
		if i%5000 == 0 {
			fmt.Printf("Sent %d requests\n", i)
			time.Sleep(1 * time.Second)
		}
	}

	time.Sleep(time.Hour)
}
```

## âš™ï¸ Configuration

### Key Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| Port | 8080 | HTTP server port |
| MaxWorkers | CPU cores * 2 | Worker pool size |
| WorkerQueueSize | 10000 | Pending job queue size |
| MaxConnections | 100000 | Maximum concurrent connections |
| ReadTimeout | 15s | Request read timeout |
| WriteTimeout | 15s | Response write timeout |
| IdleTimeout | 60s | Keep-alive idle timeout |
| ShutdownTimeout | 30s | Graceful shutdown timeout |

### Tuning Guidelines

**For High Throughput:**
- Increase `MaxWorkers` (2-4x CPU cores)
- Increase `WorkerQueueSize` (10K-50K)
- Reduce processing time per request
- Use connection pooling for downstream services

**For Low Latency:**
- Balance `MaxWorkers` with CPU cores
- Keep `WorkerQueueSize` moderate
- Optimize request processing logic
- Monitor queue depth in metrics

**For Memory Efficiency:**
- Limit `MaxConnections` based on available RAM
- Reduce `WorkerQueueSize`
- Use object pooling (sync.Pool)
- Enable memory profiling

## ðŸ—ï¸ Architecture Patterns

### 1. Worker Pool Pattern (from article)

```go
// Benefits:
// - Bounded concurrency
// - Controlled resource usage
// - Prevents goroutine explosion
// - Easy to monitor and tune

// The server uses a fixed number of workers
// Workers pull jobs from a channel queue
// Each HTTP request creates a job
// Workers process jobs concurrently
```

### 2. Channel-Based Concurrency

```go
// Communication via channels, not shared memory
// Per-request result channels
// Clean separation of concerns
// Easy to test and reason about
```

### 3. Graceful Shutdown

```go
// 1. Stop accepting new connections
// 2. Drain existing connections (with timeout)
// 3. Stop worker pool
// 4. Clean up resources
// 5. Exit
```

## ðŸ“ˆ Performance Benchmarks

### Small VPS (2 CPU, 4GB RAM) - Linode

**Worker Pool Server (8080)** - 100ms simulated work
```
wrk -t4 -c100 -d30s http://139.162.9.158:8080/
  Requests/sec:     9.84
  Avg Latency:      ~100ms (mostly simulated work)
  Transfer/sec:     1.43KB
```

**Chi Web Server (8081)** - 10ms simulated work
```
wrk -t4 -c100 -d30s http://139.162.9.158:8081/
  Requests/sec:     1,156.87
  Avg Latency:      12.34ms
  Transfer/sec:     191.57KB
```

**k6 Student Registration Test** - 2000 concurrent users
```
k6 run loadtest/k6-student-registration.js --env TARGET_URL=http://139.162.9.158:8081

  Total Requests:   217,880
  Throughput:       885 req/sec
  Avg Latency:      116.82ms
  P95 Latency:      290.69ms
  Success Rate:     100% (GET requests)
  Max VUs:          2,000
```

### Raspberry Pi 4B (4GB RAM)
- **60,000 concurrent connections**: ~800 MB RAM
- **Worker count**: 4 (matching CPU cores)
- **Request processing**: 100ms simulated work
- **Throughput**: Sustained until queue processes

### Modern Server (16 cores, 32GB RAM)
- **100,000+ concurrent connections**: ~1.2 GB RAM
- **Worker count**: 32
- **Throughput**: 5,000+ req/sec
- **Latency p95**: <200ms

## ðŸ” Monitoring & Debugging

### Enable Profiling

```go
import _ "net/http/pprof"

// In main():
go func() {
    log.Println(http.ListenAndServe("localhost:6060", nil))
}()
```

Access profiles:
```bash
# CPU profile
go tool pprof http://localhost:6060/debug/pprof/profile?seconds=30

# Memory profile
go tool pprof http://localhost:6060/debug/pprof/heap

# Goroutine profile
go tool pprof http://localhost:6060/debug/pprof/goroutine
```

### Watch Metrics in Real-Time

```bash
# Using watch
watch -n 1 'curl -s http://localhost:8080/metrics | jq'

# Using loop
while true; do 
  curl -s http://localhost:8080/metrics | jq
  sleep 2
done
```

### System Monitoring

```bash
# Monitor file descriptors
watch -n 1 'lsof -p $(pgrof -x server) | wc -l'

# Monitor connections
watch -n 1 'netstat -an | grep :8080 | grep ESTABLISHED | wc -l'

# Monitor memory
watch -n 1 'ps aux | grep server'
```

## ðŸš¨ Common Issues & Solutions

### 1. "Too many open files"
```bash
# Increase file descriptor limits (see System Requirements)
ulimit -n 1000000
```

### 2. "Cannot assign requested address"
```bash
# Increase ephemeral port range
sudo sysctl -w net.ipv4.ip_local_port_range="1024 65535"
```

### 3. High Memory Usage
- Reduce `MaxConnections`
- Implement connection limits per IP
- Add memory-based back-pressure
- Profile with pprof

### 4. Request Timeouts
- Increase worker count
- Optimize processing logic
- Increase queue size
- Add request prioritization

### 5. Slow Startup
- Pre-allocate buffers
- Warm up worker pool
- Use sync.Pool for reusable objects
- Lazy load non-critical components

## ðŸŽ“ Key Learnings from Article

1. **System Limits Matter**: File descriptors and port ranges cap your concurrency
2. **FastHTTP Wins**: 35% memory reduction through better resource reuse
3. **Worker Pool Pattern**: Balance CPU utilization without goroutine explosion
4. **Channels for Coordination**: Clean, lock-free request processing
5. **Measure Everything**: Real-time metrics guide optimization
6. **Hardware is Capable**: Even Raspberry Pi can handle 60K connections

## ðŸ“š Further Reading

- [FastHTTP Documentation](https://github.com/valyala/fasthttp)
- [Fiber Framework](https://github.com/gofiber/fiber)
- [go-chi/chi Router](https://github.com/go-chi/chi)
- [k6 Load Testing](https://k6.io/docs/)
- [Go Concurrency Patterns](https://go.dev/blog/pipelines)
- [Effective Go](https://go.dev/doc/effective_go)
- [Linux Performance Tuning](https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt)

## ðŸ“ Project Structure

```
.
â”œâ”€â”€ main.go                    # Worker Pool server (FastHTTP)
â”œâ”€â”€ cmd/
â”‚   â”œâ”€â”€ web/
â”‚   â”‚   â””â”€â”€ main.go           # Chi Web server (net/http)
â”‚   â””â”€â”€ fiber/
â”‚       â””â”€â”€ main.go           # Fiber server (FastHTTP)
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ dashboard.html        # Single server dashboard
â”‚   â”œâ”€â”€ compare.html          # 2-server comparison dashboard
â”‚   â””â”€â”€ compare3.html         # 3-server comparison dashboard
â”œâ”€â”€ loadtest/
â”‚   â”œâ”€â”€ k6-quick.js           # Quick 60s test
â”‚   â”œâ”€â”€ k6-student-registration.js  # 2000 user simulation
â”‚   â”œâ”€â”€ k6-stress.js          # Stress test (10K users)
â”‚   â””â”€â”€ k6-compare-servers.js # Compare servers
â”œâ”€â”€ Makefile                  # Build and run commands
â”œâ”€â”€ deploy.sh                 # VPS deployment script
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ VPS_DEPLOYMENT.md         # Detailed deployment guide
â””â”€â”€ DEPLOYMENT_CHEATSHEET.md  # Quick deployment reference
```

## ðŸ“ License

MIT License - feel free to use in production!

## ðŸ¤ Contributing

Contributions welcome! Areas for improvement:
- [x] Real-time metrics dashboard
- [x] Server-Sent Events (SSE)
- [x] k6 load testing scripts
- [x] Fiber server (FastHTTP comparison)
- [x] Triple server comparison dashboard
- [ ] Distributed tracing integration
- [ ] Circuit breaker pattern
- [ ] Rate limiting per IP
- [ ] Request prioritization
- [ ] Database connection pooling example
- [ ] Kubernetes deployment manifests
